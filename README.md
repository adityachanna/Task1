Hereâ€™s a humanized, copy-paste-ready version of the GitHub documentation. You can paste it directly into your README.md file.
Computation Graph with Autograd

This project implements a computation graph with autograd using Python. It enables basic operations like addition, multiplication, and ReLU activation, with automatic differentiation to compute gradients for backpropagation.
Table of Contents

    Features
    Usage
    Code Explanation
    Graph Visualization
    License

Features

    Supports addition, multiplication, and power operations.
    Implements ReLU activation function.
    Computes gradients automatically using backpropagation.
    Visualizes the computation graph using graphviz.

Usage

Here's a simple example to demonstrate how the code works:

python

from your_module import Value

# Define values
a = Value(2.0)
b = Value(-3.0)

# Perform operations
c = a + b
d = a * b + b * (a + a)
e = d.relu()

# Backpropagation
e.backward()

# Print results
print(a)  # Output: Value(data=2.0, grad=-3.0)
print(b)  # Output: Value(data=-3.0, grad=5.0)

Code Explanation
1. Value Class

The Value class represents a node in the computation graph.
Each node stores:

    data: The numerical value.
    grad: The gradient for backpropagation.
    _prev: Parent nodes (dependencies).
    _backward: A function to propagate gradients.

2. Key Operations

    Addition (+):

    python

def __add__(self, other):
    out = Value(self.data + other.data, (self, other), '+')
    def _backward():
        self.grad += out.grad
        other.grad += out.grad
    out._backward = _backward
    return out

Multiplication (*):

python

def __mul__(self, other):
    out = Value(self.data * other.data, (self, other), '*')
    def _backward():
        self.grad += other.data * out.grad
        other.grad += self.data * out.grad
    out._backward = _backward
    return out

ReLU Activation:

python

    def relu(self):
        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')
        def _backward():
            self.grad += (out.data > 0) * out.grad
        out._backward = _backward
        return out

3. Backpropagation

The backward() function computes the gradients by traversing the graph in reverse.

python

def backward(self):
    topo = []  # Store nodes in topological order
    visited = set()

    def build_topo(v):
        if v not in visited:
            visited.add(v)
            for child in v._prev:
                build_topo(child)
            topo.append(v)

    build_topo(self)
    self.grad = 1  # Start gradient at output node

    for v in reversed(topo):
        v._backward()

Graph Visualization

The computation graph can be visualized with graphviz. Below is a function to create and render the graph.

python

from graphviz import Digraph

def trace(root):
    nodes, edges = set(), set()

    def build(v):
        if v not in nodes:
            nodes.add(v)
            for child in v._prev:
                edges.add((child, v))
                build(child)

    build(root)
    return nodes, edges

def draw_dot(root):
    dot = Digraph(format='png', graph_attr={'rankdir': 'LR'})
    nodes, edges = trace(root)

    for n in nodes:
        # Node representing a value
        dot.node(name=str(id(n)), label=f"data={n.data}, grad={n.grad}", shape='record')

        if n._op:
            # Operation node
            dot.node(name=str(id(n)) + n._op, label=n._op)
            dot.edge(str(id(n)) + n._op, str(id(n)))

    for n1, n2 in edges:
        dot.edge(str(id(n1)), str(id(n2)) + n2._op)

    return dot

# Example usage
e.backward()
dot = draw_dot(e)
dot.render('computation_graph', view=True)

Contributing

Contributions are welcome! Please follow these steps:

    Fork the repository.
    Create a new branch:

    bash

git checkout -b feature-branch

Commit your changes:

bash

git commit -m "Add a new feature"

Push to your branch:

bash

    git push origin feature-branch

    Open a Pull Request.

License

This project is licensed under the MIT License. See the LICENSE file for details.
